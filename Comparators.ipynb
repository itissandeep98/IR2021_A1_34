{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sandeep/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sandeep/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,WordNetLemmatizer\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm,tnrange\n",
    "import string\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "source": [
    "# Query"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response():\n",
    "    def __init__(self,data):\n",
    "        self.data=set(data)\n",
    "\n",
    "    def getMapping(self,file):\n",
    "        self.mapping=json.load(open(file))\n",
    "        for i in self.data:\n",
    "            print(self.mapping[i])\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(len(self.data))\n",
    "\n",
    "    def add(self,response):\n",
    "        return Response(set.union(self.data,response.data))\n",
    "        \n",
    "    def intersect(self,response):\n",
    "        return Response(set.intersection(self.data,response.data))\n",
    "\n",
    "    def diff(self,response):\n",
    "        return Response(set.difference(self.data,response.data))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query():\n",
    "    def __init__(self,file):\n",
    "        self.db=json.load(open(file))\n",
    "        self.db=defaultdict(lambda:[],self.db)        \n",
    "\n",
    "    def OR(self,term1,term2):\n",
    "        return term1.add(term2)\n",
    "    \n",
    "    def AND(self,term1,term2):\n",
    "        return term1.intersect(term2)\n",
    "    \n",
    "    def ANDNOT(self,term1,term2):\n",
    "        univ=Response(np.arange(468))\n",
    "        not_term2=univ.diff(term2)\n",
    "        return term1.intersect(not_term2)\n",
    "\n",
    "    def ORNOT(self,term1,term2):\n",
    "        univ=Response(np.arange(468))\n",
    "        not_term2=univ.diff(term2)\n",
    "        joint = term1.intersect(term2)\n",
    "        return not_term2.add(joint)\n",
    "\n",
    "    def no_comparisonsOR(self,term1, term2):\n",
    "        first = self.db[term1]\n",
    "        second =self.db[term2]\n",
    "        i,j,count=0,0,0\n",
    "        ans = []\n",
    "        while(i<len(first) and j<len(second)):\n",
    "            count+=1\n",
    "            if(first[i]<second[j]):\n",
    "                ans.append(first[i])\n",
    "                i+=1          \n",
    "            elif(first[i]>second[j]):\n",
    "                ans.append(second[j])\n",
    "                j+=1\n",
    "            else:\n",
    "                ans.append(first[i])\n",
    "                i+=1\n",
    "                j+=1\n",
    "        \n",
    "        ans.extend(first[i:])\n",
    "        ans.extend(second[j:])\n",
    "\n",
    "        return count    \n",
    "\n",
    "    def no_comparisonsAND(self,term1, term2):\n",
    "        first = self.db[term1]\n",
    "        second =self.db[term2]\n",
    "        i,j,count=0,0,0\n",
    "        ans = []\n",
    "        while(i<len(first) and j<len(second)):\n",
    "            count+=1\n",
    "            if(first[i]<second[j]):\n",
    "                i+=1          \n",
    "            elif(first[i]>second[j]):\n",
    "                j+=1\n",
    "            else:\n",
    "                ans.append(first[i])\n",
    "                i+=1\n",
    "                j+=1\n",
    "        return count\n",
    "    \n",
    "    def no_comparisonsANDNOT(self,term1, term2):\n",
    "        temp=Response(np.arange(468)) \n",
    "        second = Response(self.db[term2]) #list of term2\n",
    "        second=temp.diff(second) #generating Universal - (Not terms2)\n",
    "        second=list(second.data)\n",
    "        second.sort() #sorting, if needed\n",
    "        first = list(set(self.db[term1])) #list of term1\n",
    "        first.sort()\n",
    "        #initiating general AND operator\n",
    "        i,j,count=0,0,0\n",
    "        ans = []\n",
    "        while(i<len(first) and j<len(second)):\n",
    "            count+=1\n",
    "            if(first[i]<second[j]):\n",
    "                i+=1          \n",
    "            elif(first[i]>second[j]):\n",
    "                j+=1\n",
    "            else:\n",
    "                ans.append(first[i])\n",
    "                i+=1\n",
    "                j+=1\n",
    "        return count\n",
    "    \n",
    "    def no_comparisonsORNOT(self,term1, term2):\n",
    "        temp=Response(np.arange(468)) \n",
    "        second = Response(self.db[term2]) #list of term2\n",
    "        second=temp.diff(second) #generating Universal - (Not terms2)\n",
    "        second=list(second.data)\n",
    "        second.sort() #sorting, if needed\n",
    "        first = self.db[term1] #list of term1\n",
    "    \n",
    "        i,j,count=0,0,0\n",
    "        ans = []\n",
    "        while(i<len(first) and j<len(second)):\n",
    "            count+=1\n",
    "            if(first[i]<second[j]):\n",
    "                ans.append(first[i])\n",
    "                i+=1          \n",
    "            elif(first[i]>second[j]):\n",
    "                ans.append(second[j])\n",
    "                j+=1\n",
    "            else:\n",
    "                ans.append(first[i])\n",
    "                i+=1\n",
    "                j+=1\n",
    "        ans.extend(first[i:])\n",
    "        ans.extend(second[j:])\n",
    "        return count  \n",
    "\n",
    "    def stripSpecialChar(self,text):\n",
    "        return ''.join(ch for ch in text if ch.isalnum() and not ch.isdigit() and ch not in string.punctuation)\n",
    "\n",
    "    def preProcess(self,text):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        text = text.lower()\n",
    "        text_tokens = word_tokenize(text)\n",
    "        stemmedWords = list([stemmer.stem(word) for word in text_tokens])\n",
    "\n",
    "        validTokens = [i for i in stemmedWords if i not in stopWords]\n",
    "        validTokens = [self.stripSpecialChar(x) for x in validTokens]\n",
    "        validTokens = [x for x in validTokens if len(x) > 1]\n",
    "        return validTokens\n",
    "    \n",
    "    def processQuery(self,inp,ops):\n",
    "        terms=self.preProcess(inp)\n",
    "        print(terms)\n",
    "        output=Response(self.db[terms[0]])\n",
    "        comparisons=0\n",
    "        for i in tnrange(1,len(terms)):\n",
    "            curr=Response(self.db[terms[i]])\n",
    "            if(ops[i-1]=='OR'):\n",
    "                output=self.OR(curr,output)\n",
    "                comparisons+=self.no_comparisonsOR(terms[i],terms[i-1])\n",
    "            elif(ops[i-1]=='AND'):\n",
    "                output=self.AND(curr,output)\n",
    "                comparisons+=self.no_comparisonsAND(terms[i],terms[i-1])\n",
    "            elif(ops[i-1]=='OR NOT'):\n",
    "                output=self.ORNOT(curr,output)\n",
    "                comparisons+=self.no_comparisonsORNOT(terms[i],terms[i-1])\n",
    "            elif(ops[i-1]=='AND NOT'):\n",
    "                output=self.ANDNOT(curr,output)\n",
    "                comparisons+=self.no_comparisonsANDNOT(terms[i],terms[i-1])\n",
    "\n",
    "        print(\"Number of documents matched:\",output)\n",
    "        print(\"No. of comparisons required:\",comparisons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['telephon', 'pave', 'road']\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7866fa0849a4c4686475b15c4fbf120"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of documents matched: 14\nNo. of comparisons required: 870\n"
     ]
    }
   ],
   "source": [
    "inp=\"lion stood thoughtfully for a moment\"\n",
    "ops=['OR',\"OR\",\"OR\"]\n",
    "# inp=\"telephone, paved, roads\"\n",
    "# ops=['OR NOT',\"AND NOT\"]\n",
    "\n",
    "query=Query(\"output.json\")\n",
    "query.processQuery(inp,ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
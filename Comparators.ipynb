{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sajeel\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sajeel\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,WordNetLemmatizer\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm,tnrange\n",
    "import string\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response():\n",
    "    def __init__(self,data):\n",
    "        self.data=set(data)\n",
    "\n",
    "    def getMapping(self,file):\n",
    "        '''\n",
    "         Prints out the corresponding document names from list of document IDs\n",
    "        '''\n",
    "        self.mapping=json.load(open(file))\n",
    "        for i in self.data:\n",
    "            print(self.mapping[i])\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(len(self.data))\n",
    "\n",
    "    def add(self,response):\n",
    "        '''\n",
    "        Performs the set union of given object and pass object\n",
    "        '''\n",
    "        return Response(set.union(self.data,response.data))\n",
    "        \n",
    "    def intersect(self,response):\n",
    "        '''\n",
    "        Performs the set intersection of given object with pass object\n",
    "        '''\n",
    "        return Response(set.intersection(self.data,response.data))\n",
    "\n",
    "    def diff(self,response):\n",
    "        '''\n",
    "        Performs the set difference of given object with pass object\n",
    "        '''\n",
    "        return Response(set.difference(self.data,response.data))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query():\n",
    "    def __init__(self,file):\n",
    "        '''\n",
    "        initializes the object with loading the index file\n",
    "        '''\n",
    "        self.db=json.load(open(file))\n",
    "        self.db=defaultdict(lambda:[],self.db)        \n",
    "\n",
    "    def OR(self,term1,term2):\n",
    "        '''\n",
    "        Finds the docs after applying OR operation on given list of documents\n",
    "        '''\n",
    "        return term1.add(term2)\n",
    "    \n",
    "    def AND(self,term1,term2):\n",
    "        '''\n",
    "        Finds the docs after applying AND operation on given list of documents\n",
    "        '''\n",
    "        return term1.intersect(term2)\n",
    "    \n",
    "    def ANDNOT(self,term1,term2):\n",
    "        '''\n",
    "        Finds the docs after applying AND NOT operation on given list of documents\n",
    "        '''\n",
    "        univ=Response(np.arange(467))\n",
    "        not_term2=univ.diff(term2)\n",
    "        return term1.intersect(not_term2)\n",
    "\n",
    "    def ORNOT(self,term1,term2):\n",
    "        '''\n",
    "        Finds the docs after applying OR NOT operation on given list of documents\n",
    "        '''\n",
    "        univ=Response(np.arange(467))\n",
    "        not_term2=univ.diff(term2)\n",
    "        joint = term1.intersect(term2)\n",
    "        return not_term2.add(joint)\n",
    "\n",
    "    def count(self,first,second):\n",
    "        i,j,count=0,0,0\n",
    "        while(i<len(first) and j<len(second)):\n",
    "            count+=1\n",
    "            if(first[i]<second[j]):\n",
    "                i+=1          \n",
    "            elif(first[i]>second[j]):\n",
    "                j+=1\n",
    "            else:\n",
    "                i+=1\n",
    "                j+=1\n",
    "        return count\n",
    "\n",
    "    def no_comparisonsOR(self,term1, term2):\n",
    "        '''\n",
    "        To return the number of comparisons it will make in OR operations between two list of documents\n",
    "        '''\n",
    "        first = list(term1.data)\n",
    "        first.sort()\n",
    "        second =list(term2.data)\n",
    "        second.sort()\n",
    "        return self.count(first,second)    \n",
    "\n",
    "    def no_comparisonsAND(self,term1, term2):\n",
    "        '''\n",
    "        To return the number of comparisons it will make in AND operations between two list of documents\n",
    "        '''\n",
    "        first = list(term1.data)\n",
    "        first.sort()\n",
    "        second =list(term2.data)\n",
    "        second.sort()\n",
    "        return self.count(first,second) \n",
    "    \n",
    "    def no_comparisonsANDNOT(self,term1, term2):\n",
    "        '''\n",
    "        To return the number of comparisons it will make in AND NOT operations between two list of documents\n",
    "        '''\n",
    "        first = list(term1.data)\n",
    "        first.sort()\n",
    "        univ=Response(np.arange(467)) \n",
    "        not_term2=univ.diff(term2) \n",
    "        second=list(not_term2.data)\n",
    "        second.sort()\n",
    "        return self.count(first,second) \n",
    "    \n",
    "    def no_comparisonsORNOT(self,term1, term2):\n",
    "        '''\n",
    "        To return the number of comparisons it will make in OR NOT operations between two list of documents\n",
    "        '''\n",
    "        first = list(term1.data)\n",
    "        first.sort()\n",
    "        univ=Response(np.arange(467)) \n",
    "        not_term2=univ.diff(term2) \n",
    "        second=list(not_term2.data)\n",
    "        second.sort()\n",
    "    \n",
    "        return self.count(first,second)  \n",
    "\n",
    "    def stripSpecialChar(self,text):\n",
    "        return ''.join(ch for ch in text if ch.isalnum() and not ch.isdigit() and ch not in string.punctuation)\n",
    "\n",
    "    def preProcess(self,text):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        text = text.lower()                                     # convert all text to lower case\n",
    "        text_tokens = word_tokenize(text)                       # tokenizing the text\n",
    "\n",
    "        # stemmedWords = list([stemmer.stem(word) for word in text_tokens])\n",
    "        # validTokens = [i for i in stemmedWords if i not in stopWords]\n",
    "\n",
    "        validTokens = [i for i in text_tokens if i not in stopWords]    # removing stop words\n",
    "\n",
    "        validTokens = [self.stripSpecialChar(x) for x in validTokens]   # stripping special characters\n",
    "        validTokens = [x for x in validTokens if len(x) > 1]    # Choosing only words which has length > 1\n",
    "        return validTokens\n",
    "    \n",
    "    def processQuery(self,inp,ops):\n",
    "        '''\n",
    "        Performs query with given string and list of operations\n",
    "        '''\n",
    "        terms=self.preProcess(inp)\n",
    "        print(terms)\n",
    "        output=Response(self.db[terms[0]])\n",
    "        comparisons=0\n",
    "        for i in tnrange(1,len(terms)):\n",
    "            curr=Response(self.db[terms[i]])\n",
    "            if(ops[i-1]=='OR'):\n",
    "                output=self.OR(output, curr)\n",
    "                comparisons+=self.no_comparisonsOR(output,curr)\n",
    "            elif(ops[i-1]=='AND'):\n",
    "                output=self.AND(output, curr)\n",
    "                comparisons+=self.no_comparisonsAND(output,curr)\n",
    "            elif(ops[i-1]=='OR NOT'):\n",
    "                output=self.ORNOT(output, curr)\n",
    "                comparisons+=self.no_comparisonsORNOT(output,curr)\n",
    "            elif(ops[i-1]=='AND NOT'):\n",
    "                output=self.ANDNOT(output, curr)\n",
    "                comparisons+=self.no_comparisonsANDNOT(output,curr)\n",
    "            else:\n",
    "                raise Exception(\"Operand not Identified:\"+ops[i-1])\n",
    "\n",
    "        print(\"Number of documents matched:\",output)\n",
    "        print(\"No. of comparisons required:\",comparisons)\n",
    "        docs_list = list(output.data)\n",
    "        docs_list.sort()\n",
    "        print(docs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of queries:1\n",
      "Enter the sentence:-SNOWBANK, STUBBED, LEGIONS\n",
      "Enter the operands:-and not, or\n",
      "['snowbank', 'stubbed', 'legions']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a749c6088374494090ec9645601c703b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of documents matched: 2\n",
      "No. of comparisons required: 2\n",
      "[64, 127]\n"
     ]
    }
   ],
   "source": [
    "n = int(input(\"Enter the number of queries:\"))\n",
    "for _ in range(n):\n",
    "    sentence_query = input(\"Enter the sentence:-\")\n",
    "    \n",
    "    # Performing preprocessing(splitting, uppercase, stripping space from endpoints) on the operand input\n",
    "    operands_=list(map(str.strip,input(\"Enter the operands:-\").upper().split(\",\")))\n",
    "\n",
    "    query=Query(\"output.json\")\n",
    "    query.processQuery(sentence_query,operands_)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

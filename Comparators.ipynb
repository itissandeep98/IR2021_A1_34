{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sandeep/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/sandeep/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,WordNetLemmatizer\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm,tnrange\n",
    "import string\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "source": [
    "# Query"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response():\n",
    "    def __init__(self,data):\n",
    "        self.data=set(data)\n",
    "\n",
    "    def getMapping(self,file):\n",
    "        self.mapping=json.load(open(file))\n",
    "        for i in self.data:\n",
    "            print(self.mapping[i])\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(len(self.data))\n",
    "\n",
    "    def add(self,response):\n",
    "        return Response(set.union(self.data,response.data))\n",
    "        \n",
    "    def intersect(self,response):\n",
    "        return Response(set.intersection(self.data,response.data))\n",
    "\n",
    "    def diff(self,response):\n",
    "        return Response(set.difference(self.data,response.data))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query():\n",
    "    def __init__(self,file):\n",
    "        self.db=json.load(open(file))\n",
    "        self.db=defaultdict(lambda:[],self.db)        \n",
    "\n",
    "    def OR(self,term1,term2):\n",
    "        return term1.add(term2)\n",
    "    \n",
    "    def AND(self,term1,term2):\n",
    "        return term1.intersect(term2)\n",
    "    \n",
    "    def ANDNOT(self,term1,term2):\n",
    "        # temp=Response(set(sum(list(self.db.copy().values()),[])))\n",
    "        temp=Response(np.arange(468))\n",
    "        temp1=temp.diff(term2)\n",
    "        return term1.intersect(temp1)\n",
    "\n",
    "    def ORNOT(self,term1,term2):\n",
    "        # temp=Response(set(sum(list(self.db.copy().values()),[])))\n",
    "        temp=Response(np.arange(468))\n",
    "        temp1=temp.diff(term2)\n",
    "        temp2 = term1.intersect(term2)\n",
    "        return temp1.add(temp2)\n",
    "\n",
    "    def no_comparisonsOR(self,term1, term2):\n",
    "        first = self.db[term1]\n",
    "        second =self.db[term2]\n",
    "        i,j,count=0,0,0\n",
    "        ans = []\n",
    "        while(i<len(first) and j<len(second)):\n",
    "            count+=1\n",
    "            if(first[i]<second[j]):\n",
    "                ans.append(first[i])\n",
    "                i+=1          \n",
    "            elif(first[i]>second[j]):\n",
    "                ans.append(second[j])\n",
    "                j+=1\n",
    "            else:\n",
    "                ans.append(first[i])\n",
    "                i+=1\n",
    "                j+=1\n",
    "        \n",
    "        ans.extend(first[i:])\n",
    "        ans.extend(second[j:])\n",
    "\n",
    "        return count    \n",
    "\n",
    "    def no_comparisonsAND(self,term1, term2):\n",
    "        first = self.db[term1]\n",
    "        second =self.db[term2]\n",
    "        i,j,count=0,0,0\n",
    "        ans = []\n",
    "        while(i<len(first) and j<len(second)):\n",
    "            count+=1\n",
    "            if(first[i]<second[j]):\n",
    "                i+=1          \n",
    "            elif(first[i]>second[j]):\n",
    "                j+=1\n",
    "            else:\n",
    "                ans.append(first[i])\n",
    "                i+=1\n",
    "                j+=1\n",
    "        return count\n",
    "    \n",
    "    def no_comparisonsANDNOT(self,term1, term2):\n",
    "        temp=Response(np.arange(468)) \n",
    "        second = Response(self.db[term2]) #list of term2\n",
    "        second=temp.diff(second) #generating Universal - (Not terms2)\n",
    "        second=list(second.data)\n",
    "        second.sort() #sorting, if needed\n",
    "        first = list(set(self.db[term1])) #list of term1\n",
    "        first.sort()\n",
    "        #initiating general AND operator\n",
    "        i,j,count=0,0,0\n",
    "        ans = []\n",
    "        while(i<len(first) and j<len(second)):\n",
    "            count+=1\n",
    "            if(first[i]<second[j]):\n",
    "                i+=1          \n",
    "            elif(first[i]>second[j]):\n",
    "                j+=1\n",
    "            else:\n",
    "                ans.append(first[i])\n",
    "                i+=1\n",
    "                j+=1\n",
    "        return count\n",
    "    \n",
    "    def no_comparisonsORNOT(self,term1, term2):\n",
    "        temp=Response(np.arange(468)) \n",
    "        second = Response(self.db[term2]) #list of term2\n",
    "        second=temp.diff(second) #generating Universal - (Not terms2)\n",
    "        second=list(second.data)\n",
    "        second.sort() #sorting, if needed\n",
    "        first = self.db[term1] #list of term1\n",
    "    \n",
    "        i,j,count=0,0,0\n",
    "        ans = []\n",
    "        while(i<len(first) and j<len(second)):\n",
    "            count+=1\n",
    "            if(first[i]<second[j]):\n",
    "                ans.append(first[i])\n",
    "                i+=1          \n",
    "            elif(first[i]>second[j]):\n",
    "                ans.append(second[j])\n",
    "                j+=1\n",
    "            else:\n",
    "                ans.append(first[i])\n",
    "                i+=1\n",
    "                j+=1\n",
    "        while(i<len(first)):\n",
    "            ans.append(first[i])\n",
    "            i+=1\n",
    "        while(j<len(second)):\n",
    "            ans.append(second[j])\n",
    "            j+=1\n",
    "        return count  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=Query(\"output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7ec9dbb13a34555bea48671bd2a26b1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19\n25\n"
     ]
    }
   ],
   "source": [
    "queries=[\"stink\",\"middle\"]\n",
    "\n",
    "output=Response(query.db[queries[0]])\n",
    "\n",
    "for i in tnrange(1,len(queries)):\n",
    "    curr=Response(query.db[queries[i]])\n",
    "    output=query.OR(curr,output)\n",
    "    print(query.no_comparisonsAND(queries[i],queries[i-1]))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stories/grav\nstories/quest\nstories/bumm\nstories/rocket.sf\nstories/deathmrs.d\nstories/zombies.txt\nstories/gulliver.txt\nstories/darkness.txt\nstories/crazy.hum\nstories/hellmach.txt\nstories/fic1\nstories/long1-3.txt\nstories/elite.app\nstories/nigel.5\nstories/forgotte\nstories/nihgel_8.9\nstories/aesop11.txt\nstories/sick-kid.txt\nstories/retrib.txt\nstories/keepmodu.txt\nstories/beggars.txt\nstories/girlclub.txt\nstories/friends.txt\nstories/hitch2.txt\nstories/valen\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0,\n",
       " 24,\n",
       " 30,\n",
       " 64,\n",
       " 112,\n",
       " 132,\n",
       " 143,\n",
       " 145,\n",
       " 155,\n",
       " 161,\n",
       " 164,\n",
       " 179,\n",
       " 202,\n",
       " 228,\n",
       " 232,\n",
       " 251,\n",
       " 266,\n",
       " 268,\n",
       " 287,\n",
       " 298,\n",
       " 300,\n",
       " 323,\n",
       " 337,\n",
       " 371,\n",
       " 439}"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "output.getMapping(\"mapping.json\")\n",
    "output.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}